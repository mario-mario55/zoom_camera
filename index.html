<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>顔ズームカメラ（クロスフェード付き）</title>
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
  <style>
    html, body {
      margin: 0;
      padding: 0;
      overflow: hidden;
      background: black;
      width: 100%;
      height: 100%;
    }

    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
      object-fit: cover;
      width: 100vw;
      height: 100vh;
    }

    #switchBtn {
      position: absolute;
      z-index: 10;
      top: 10px;
      right: 10px;
      padding: 10px;
      font-size: 16px;
      border-radius: 8px;
    }
  </style>
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="canvas"></canvas>
  <audio id="bgm" src="bgm.mp3" loop></audio>
  <button id="switchBtn">カメラ切替</button>

  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const bgm = document.getElementById('bgm');

    let state = 'idle'; // idle | zooming | locked
    let faceBox = null;
    let zoom = 1.0;
    let zoomTarget = 2.5;
    let zoomDuration = 3000;
    let zoomStartTime = null;

    let useFrontCamera = false;
    let currentStream;

    async function loadModels() {
      await faceapi.nets.tinyFaceDetector.loadFromUri('./models/tiny_face_detector');
    }

    async function startCamera() {
      if (currentStream) {
        currentStream.getTracks().forEach(track => track.stop());
      }

      try {
        const constraints = {
          video: {
            facingMode: useFrontCamera ? "user" : "environment"
          }
        };
        currentStream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = currentStream;
      } catch (err) {
        console.error("カメラ起動エラー:", err);
        alert("カメラを起動できませんでした。");
      }
    }

    async function detectRandomFace() {
      const options = new faceapi.TinyFaceDetectorOptions({ inputSize: 416, scoreThreshold: 0.3 });
      const detections = await faceapi.detectAllFaces(video, options);
      if (detections.length === 0) return null;
      const randomIndex = Math.floor(Math.random() * detections.length);
      return detections[randomIndex].box;
    }

    function startZoom(box) {
      faceBox = box;
      zoom = 1.0;
      zoomStartTime = performance.now();
      state = 'zooming';
      bgm.currentTime = 0;
      bgm.play();
    }

    function renderZoomedVideo(currentZoom, alpha = 1.0) {
      const { x, y, width, height } = faceBox;
      ctx.save();
      ctx.globalAlpha = alpha;
      ctx.drawImage(
        video,
        x, y, width, height,
        canvas.width / 2 - (width * currentZoom) / 2,
        canvas.height / 2 - (height * currentZoom) / 2,
        width * currentZoom,
        height * currentZoom
      );
      ctx.restore();
    }

    function drawFaceBoxes() {
      const options = new faceapi.TinyFaceDetectorOptions({ inputSize: 224 });
      faceapi.detectAllFaces(video, options).then(faces => {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        for (const face of faces) {
          const { x, y, width, height } = face.box;
          ctx.strokeStyle = 'lime';
          ctx.lineWidth = 2;
          ctx.strokeRect(x, y, width, height);
        }
      });
    }

    function render(timestamp) {
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;

      if (state === 'idle') {
        drawFaceBoxes();
      } else if (state === 'zooming') {
        const elapsed = timestamp - zoomStartTime;
        const t = Math.min(elapsed / zoomDuration, 1.0);
        zoom = 1.0 + (zoomTarget - 1.0) * easeInOutQuad(t);

        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        renderZoomedVideo(zoom, t);

        if (t >= 1.0) {
          state = 'locked';
          bgm.pause();
        }
      } else if (state === 'locked') {
        renderZoomedVideo(zoomTarget);
      }

      requestAnimationFrame(render);
    }

    function easeInOutQuad(t) {
      return t < 0.5 ? 2 * t * t : -1 + (4 - 2 * t) * t;
    }

    canvas.addEventListener('click', async () => {
      if (state === 'idle') {
        const face = await detectRandomFace();
        if (face) startZoom(face);
      } else if (state === 'locked') {
        state = 'idle';
        bgm.pause();
      }
    });

    window.addEventListener('load', async () => {
      await loadModels();
      await startCamera();
      render();
    });

    document.getElementById('switchBtn').addEventListener('click', () => {
      useFrontCamera = !useFrontCamera;
      startCamera();
    });
  </script>
</body>
</html>

